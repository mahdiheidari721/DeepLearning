{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xku5LoXVqcxR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import convolve2d\n",
        "from torch import tensor, Tensor\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-PbKOk6Qrhac"
      },
      "outputs": [],
      "source": [
        "def generate(x, y):\n",
        "    x_c = x.clone()\n",
        "    x_c[:, :10] *= 0.0\n",
        "    x_c[range(x.shape[0]), y] = x.max()\n",
        "    return x_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gIXx9S2Jshdh"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        self.linear_classifier = LinearClassifier(sum(dims)-784, 10)\n",
        "        self.linear_classifier.to(\"cuda\")\n",
        "\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layers += [Layer(dims[d], dims[d + 1]).cuda()]\n",
        "\n",
        "    def predict(self, x):\n",
        "        goodness_per_label = []\n",
        "        for label in range(10):\n",
        "            h = generate(x, label)\n",
        "            goodness = []\n",
        "            for layer in self.layers:\n",
        "                h = layer(h)\n",
        "                goodness += [h.pow(2).mean(1)]\n",
        "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
        "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
        "        return goodness_per_label.argmax(1)\n",
        "\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print('training layer', i, '...')\n",
        "            h_pos, h_neg = layer.train(h_pos, h_neg)\n",
        "    def train_last_layer(self,x,y):\n",
        "        outputs = []\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            outputs.append(x)\n",
        "        concatenated_output = torch.cat(outputs, dim=1)\n",
        "        self.linear_classifier.train_weights(concatenated_output, y)\n",
        "    def predictu(self,x):\n",
        "      outputs = []\n",
        "      for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            outputs.append(x)\n",
        "      concatenated_output = torch.cat(outputs, dim=1)\n",
        "      return self.linear_classifier.predict(concatenated_output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D4Ss5TZ5u334"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "    def train_weights(self, x, y, num_epochs=100):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            outputs = self(x)\n",
        "            probabilities = nn.functional.softmax(outputs, dim=1)\n",
        "            _, predicted_class = torch.max(probabilities, 1)\n",
        "        return predicted_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oAW-A5pap04m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "class Layer(nn.Linear):\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 bias=True, device=None, dtype=None):\n",
        "        super().__init__(in_features, out_features, bias, device, dtype)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "        self.opt = Adam(self.parameters(), lr=0.02)\n",
        "        self.threshold = 2.0\n",
        "        self.num_epochs = 1000\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "        return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        for i in range(self.num_epochs):\n",
        "            g_pos = self.forward(x_pos).pow(2).mean(1)\n",
        "            g_neg = self.forward(x_neg).pow(2).mean(1)\n",
        "            loss = torch.log(1 + torch.exp(torch.cat([self.threshold - g_pos, g_neg - self.threshold]))).mean()\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OPE1wcqzm8Z"
      },
      "source": [
        "#Unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y0mMvp9sQdxE"
      },
      "outputs": [],
      "source": [
        "def create_mask(shape, iterations: int = 10):\n",
        "\n",
        "    blur_filter_1 = np.array(((0, 0, 0), (0.25, 0.5, 0.25), (0, 0, 0)))\n",
        "    blur_filter_2 = blur_filter_1.T\n",
        "\n",
        "    image = np.random.randint(0, 2, size=shape)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        image = np.abs(convolve2d(image, blur_filter_1, mode='same') / blur_filter_1.sum())\n",
        "        image = np.abs(convolve2d(image, blur_filter_2, mode='same') / blur_filter_2.sum())\n",
        "\n",
        "    mask = np.round(image).astype(np.uint8)\n",
        "\n",
        "    return tensor(mask).to(\"cuda:0\")\n",
        "\n",
        "\n",
        "def create_negative_image(image_1: Tensor, image_2: Tensor):\n",
        "\n",
        "    mask = create_mask((image_1.shape[0], image_1.shape[1]))\n",
        "\n",
        "    image_1 = torch.mul(image_1, mask)\n",
        "    image_2 = torch.mul(image_2, 1 - mask)\n",
        "\n",
        "    return torch.add(image_1, image_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Utls_4Ozmfn",
        "outputId": "ed034d7a-f80f-45b6-a6d7-54e3c7406b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 93189622.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 28952125.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28784910.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2264415.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize((0.1300,), (0.3000,)),\n",
        "        Lambda(lambda x: torch.flatten(x))])\n",
        "\n",
        "train_loader = DataLoader(MNIST('./data/', train=True,download=True,transform=transform),batch_size=50000, shuffle=False)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    MNIST('./data/', train=False,\n",
        "          download=True,\n",
        "          transform=transform),\n",
        "    batch_size=10000, shuffle=False)\n",
        "shuffled_train_loader = DataLoader(MNIST('./data/', train=True,download=True,transform=transform),batch_size=50000, shuffle=True)\n",
        "\n",
        "z, q = next(iter(shuffled_train_loader))\n",
        "z, q = z.cuda(), q.cuda()\n",
        "x, y = next(iter(train_loader))\n",
        "x, y = x.cuda(), y.cuda()\n",
        "x_neg2=create_negative_image(x, z)\n",
        "\n",
        "x_neg2 = x_neg2.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBFDYu58aSFA",
        "outputId": "1badbf8f-3430-47c2-d770-d5746034bc8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training layer 0 ...\n",
            "training layer 1 ...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "netu = Net([784, 2000,2000])\n",
        "netu = netu.to(device)\n",
        "netu.train(x, x_neg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx-ma062zNhV",
        "outputId": "80485e99-8d9b-4d6e-939c-8da43ca62ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/100], Loss: 0.5712\n",
            "Epoch [20/100], Loss: 0.3775\n",
            "Epoch [30/100], Loss: 0.3274\n",
            "Epoch [40/100], Loss: 0.2798\n",
            "Epoch [50/100], Loss: 0.2530\n",
            "Epoch [60/100], Loss: 0.2339\n",
            "Epoch [70/100], Loss: 0.2207\n",
            "Epoch [80/100], Loss: 0.2104\n",
            "Epoch [90/100], Loss: 0.2022\n",
            "Epoch [100/100], Loss: 0.1954\n",
            "train error: 0.06192004680633545\n",
            "train accuracy: 0.9380799531936646\n",
            "test error: 0.0667000412940979\n",
            "test accuracy: 0.9332999587059021\n"
          ]
        }
      ],
      "source": [
        "netu.train_last_layer(x,y)\n",
        "\n",
        "print('train error:', 1.0 - netu.predictu(x).eq(y).float().mean().item())\n",
        "train_accuracy = netu.predictu(x).eq(y).float().mean().item()\n",
        "print('train accuracy:', train_accuracy)\n",
        "\n",
        "x_te, y_te = next(iter(test_loader))\n",
        "x_te, y_te = x_te.cuda(), y_te.cuda()\n",
        "\n",
        "print('test error:', 1.0 - netu.predictu(x_te).eq(y_te).float().mean().item())\n",
        "test_accuracy = netu.predictu(x_te).eq(y_te).float().mean().item()\n",
        "print('test accuracy:', test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "به همانند قسمت قبل تابع لاس را تعریف می کنیم.\n",
        "\n",
        "و وقتی از لیبل ها استفاده نمیکنیم اما همچنان داده غلط و درست داریم و هر بچ داده درست و غلط زا با هم می دهیم تا مذل اموزش داده شود.\n",
        "\n",
        "در واقع در اینجا یک مدلی آموزش داده می شود بطوریکه اگر ورودی به آن بدهیم خروجی می شود خروجی های هر لایه که با هم کانکت شدند.\n",
        "این می شود خروجی یک شبکه که با لیبل ها ارتباط خوبی دارد و عکس های تا حدی شبیه به هم خروجی هایی تا حدی شبیه به هم دارند و تا حدی نورون های مشابه ای بیشتر فعال می شوند.\n",
        "\n",
        "حال که می دانیم تا حدی بردار های خروجی به ازای عکس های ورودی از یک کلاس شبیه به هم می باشند کافیست یک کلاسیفایر اموزش بدهیم تا تشخیص بدهید چه بردار های در جه زمینه هایی مربوط به چه کلاس هایی هستند\n",
        "\n",
        "\n",
        "و همانطور که در کد دیده می شود در نهایت یک کلاس لینیر که یک لایه هست و در نهایت بردار ای می دهد که از سافت مکس رد می شود اموزش داده می شود.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "در نهایت هر داده دیگر اول از شبکه\n",
        "FF\n",
        "رد می شود تا وکتور خروجی آن ساخته شود\n",
        "بعد از آن به لایه کلاسیفایر اموزش داده شده داده می شود و یک لیبل خروجی داده می شود."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
